{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "262ee4f1cc793d5cc0302d24cbc64461",
     "grade": false,
     "grade_id": "cell-aa820d6aaf4304db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "413d0ac6b638fbbb059024aa479f3b66",
     "grade": false,
     "grade_id": "cell-24e63ee011a83003",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "# Assignment 1 Part 3: Hidden Markov Models (HMMs) (30 pts)\n",
    "\n",
    "In this assignment, we're going to train a Hidden Markov Model (HMM) that is able to tag words with their [part-of-speech](https://en.wikipedia.org/wiki/Part_of_speech) (POS) in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42c6f82b52e85920cee3cca49b7239a",
     "grade": false,
     "grade_id": "cell-0153dc3ed86e1f61",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk_data_path = \"assets/nltk_data\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16f36765718581e9df85de6580f6a282",
     "grade": false,
     "grade_id": "cell-d63bebc6fef0f0d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1: Load the dataset (20 pts)\n",
    "\n",
    "Let's first load some POS-tagged data for training and testing our HMM. **Complete the `load_data` function below** that takes two arguments, `data_file`, which specifies the data file to read, and `label`, which indicates whether to include labels in your data.\n",
    "\n",
    "Under the `assets/conll03` folder, you are given three data files, `eng.train`, `eng.testa` and `eng.testb`, that come from a [CoNLL-2003 shared task](https://www.clips.uantwerpen.be/conll2003/ner/). The shared task was originally held as a competition for Named Entity Recognition (NER), but the data it provided also includes POS tags that make POS Tagging possible. NER and POS Tagging are very similar tasks and HMMs are capable of handling both of them. After finishing this assignment, you are encouraged to re-purpose your HMM tagger for NER as a self-exercise. \n",
    "\n",
    "All three data files share the same format. They are concatenation of several documents demarcated by `-DOCSTART- -X- O O`. Sentences are separated by empty lines and each token occupies one line with its POS tag. For example, \n",
    "\n",
    "`scientists NNS I-NP O`\n",
    "\n",
    "says the token `scientists` has a POS tag of `NNS` (noun, plural). If interested, see [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for a reference list of all POS tags. The [CoNLL-2003 shared task](https://www.clips.uantwerpen.be/conll2003/ner/) page also includes some descriptions of the data. \n",
    "\n",
    "In many NLP tasks, it's commonplace to divide a dataset into training, development and testing sets. Since training and development sets are used for model learning and hyper-parameter tuning, they often include labels (if any). However, labels are often *not* distributed with testing sets to prevent anyone from cheating, especially in competitions. Here we will follow the same convention, by taking `eng.train` as the training set, `eng.testa` as the development set and `eng.testb` as the testing set, and **will only allow the training and development sets to include labels** by specifying the `label` argument: \n",
    "\n",
    "* **When called with `label=True`**, your function should return a `list`, where each element is a `list` of `tuple` representing each sentence. Each `tuple` contains a token and its POS tag, such as `(\"scientists\", \"NNS\")`. \n",
    "\n",
    "* **When called with `label=False`**, your function should also return a `list`, where each element is just a `list` of tokens. POS tags are not included and `tuple` is not needed.\n",
    "\n",
    "Finally, **in all the three data files, we only consider \"substantial\" sentences that have at least $10$ tokens**. You'd need to filter out shorter sentences and should not include them in any of your three subsets of the data. \n",
    "\n",
    "**This function should return a `list`, where each element is either a `list` of `tuple` or a `list` of `str`.**\n",
    "\n",
    "**When the argument `label=True`, an example output would be:**\n",
    "```\n",
    "[[('The', 'DT'), ('European', 'NNP'), ('Commission', 'NNP'), ('said', 'VBD'), ('on', 'IN'), ('Thursday', 'NNP'), ('it', 'PRP'), ('disagreed', 'VBD'), ('with', 'IN'), ('German', 'JJ'), ('advice', 'NN'), ('to', 'TO'), ('consumers', 'NNS'), ('to', 'TO'), ('shun', 'VB'), ('British', 'JJ'), ('lamb', 'NN'), ('until', 'IN'), ('scientists', 'NNS'), ('determine', 'VBP'), ('whether', 'IN'), ('mad', 'JJ'), ('cow', 'NN'), ('disease', 'NN'), ('can', 'MD'), ('be', 'VB'), ('transmitted', 'VBN'), ('to', 'TO'), ('sheep', 'NN'), ('.', '.')], \n",
    "...\n",
    "[('Africa', 'NNP'), (')', ')'), ('71', 'CD'), ('74', 'CD'), ('75', 'CD'), (',', ','), ('David', 'NNP'), ('Gilford', 'NNP'), ('69', 'CD'), ('74', 'CD'), ('77', 'CD'), ('.', '.')]]\n",
    "```\n",
    "\n",
    "**When the argument `label=False`, an example output would be:**\n",
    "\n",
    "```\n",
    "[['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.'], \n",
    " ...\n",
    " ['The', 'lanky', 'former', 'Leeds', 'United', 'defender', 'did', 'not', 'make', 'his', 'England', 'debut', 'until', 'the', 'age', 'of', '30', 'but', 'eventually', 'won', '35', 'caps', 'and', 'was', 'a', 'key', 'member', 'of', 'the', '1966', 'World', 'Cup', 'winning', 'team', 'with', 'his', 'younger', 'brother', ',', 'Bobby', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a05e5919d07bf0c5744f22003673ecef",
     "grade": false,
     "grade_id": "cell-876ee5c9fdc4f347",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_file, label=True):\n",
    "    \"\"\"\n",
    "    Load tokens (and labels, if allowed) from a data_file\n",
    "    \"\"\"\n",
    "    \n",
    "    tagged_sents = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a84b7996b68bdc6094c4f70973845331",
     "grade": false,
     "grade_id": "cell-7896bc924dc5f94b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's create the training, development and testing sets using load_data above\n",
    "# Remember *not* to include labels in the testing set\n",
    "\n",
    "dataset = {\"train\": None, \"dev\": None, \"test\": None}\n",
    "\n",
    "# Fill in the \"blank\" - replace the \"None\" above with the correct dataset\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5580b65bf240a3f12706d245c832bd63",
     "grade": true,
     "grade_id": "cell-fe18fb79a414feb2",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Q1: Your train set should be a list. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/3312953428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Q1: Your {subset} set should be a list. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7146\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1783\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1516\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Q1: Your train set should be a list. "
     ]
    }
   ],
   "source": [
    "# Autograder tests\n",
    "\n",
    "for subset in dataset:\n",
    "    assert isinstance(dataset[subset], list), f\"Q1: Your {subset} set should be a list. \"\n",
    "\n",
    "for subset, length in zip(dataset.keys(), [7146, 1783, 1516]):\n",
    "    assert len(dataset[subset]) == length, f\"Q1: Your {subset} set is of an incorrect length. \"\n",
    "\n",
    "for subset in dataset:\n",
    "    for index, sent in enumerate(dataset[subset]):\n",
    "        assert len(sent) >= 10, f\"Q1: Sentence at index {index} of your {subset} set is shorter than required. \"\n",
    "        item_type = tuple if subset in [\"train\", \"dev\"] else str\n",
    "        for item in sent:\n",
    "            assert isinstance(item, item_type), f\"Q1: Your {subset} set should contain lists of {item_type.__name__}s. \"\n",
    "\n",
    "# Some hidden tests below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4bb992860534d58099541abb021c8efe",
     "grade": false,
     "grade_id": "cell-902e515e7633ecbd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2: Train an HMM Tagger (10 pts)\n",
    "\n",
    "Once we have the training, development and testing sets ready, it's straightforward to train an HMM tagger with the help of the `nltk` library. Specifically, check out the [documentation](http://www.nltk.org/api/nltk.tag.html?#nltk.tag.hmm.HiddenMarkovModelTagger) of the `HiddenMarkovModelTagger` class and understand how to use its classmethod `train` to create an HMM tagger. The classmethod `train` also accepts an argument called `test_sequence`, where you can plug in your development data and get a score as an estimate for how well your HMM will perform on unseen data. \n",
    "\n",
    "**This function should return a trained `HiddenMarkovModelTagger` object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb52a82bda5a8c46372864d43a8a1710",
     "grade": false,
     "grade_id": "cell-be2a75b95ae4c066",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "def train_hmm(train, dev):\n",
    "    \"\"\"\n",
    "    Train an HMM tagger on the training set provided\n",
    "    \"\"\"\n",
    "    \n",
    "    hmm_tagger = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return hmm_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an HMM tagger, which takes a while\n",
    "\n",
    "hmm_tagger = train_hmm(dataset[\"train\"], dataset[\"dev\"])\n",
    "\n",
    "# The number it reports is the accuracy on the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fb29e6b896b36f7cead22f3c27b0980",
     "grade": true,
     "grade_id": "cell-bd592f77659516e2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Q2: Your function should return a HiddenMarkovModelTagger. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/2222183138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Autograder tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmm_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHiddenMarkovModelTagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Q2: Your function should return a HiddenMarkovModelTagger. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Q2: Your HMM's states are empty. The tagger wasn't trained properly. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Q2: Your function should return a HiddenMarkovModelTagger. "
     ]
    }
   ],
   "source": [
    "# Autograder tests\n",
    "\n",
    "assert isinstance(hmm_tagger, HiddenMarkovModelTagger), \"Q2: Your function should return a HiddenMarkovModelTagger. \"\n",
    "assert hmm_tagger._states, \"Q2: Your HMM's states are empty. The tagger wasn't trained properly. \"\n",
    "\n",
    "# Some hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86b1c123f41591a193a476dc8778b00a",
     "grade": false,
     "grade_id": "cell-4b22a356806c732d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have trained an HMM tagger, let's now apply it to your testing set to see how it does. For example, the line of code below tags the first sentence in the testing set. Feel free to adapt the code to tag more sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/1125118782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tag'"
     ]
    }
   ],
   "source": [
    "hmm_tagger.tag(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_data_mining_ii_v1_assignment1_part3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
